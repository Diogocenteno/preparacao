{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_UoIgg7-sFB"
      },
      "outputs": [],
      "source": [
        "# 1 Importações necessárias\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, month\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, Normalizer, PCA\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Inicializar a sessão Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leitura Parquet\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Carregar o arquivo Parquet (substitua pelo caminho correto)\n",
        "df_video = spark.read.parquet(\"C:/Users/diogo/Downloads/videos-comments-tratados.snappy.parquet\")\n",
        "\n",
        "# Mostrar as primeiras 10 linhas\n",
        "df_video.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvDWBo4t-vTa"
      },
      "outputs": [],
      "source": [
        "# 2. Leia o arquivo Parquet\n",
        "df_video = spark.read.parquet(\"C:/Users/diogo/Downloads/videos-comments-tratados.snappy.parquet\")\n",
        "print(\"Dados carregados:\")\n",
        "df_video.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6XroL6R-76p"
      },
      "outputs": [],
      "source": [
        "# 3. Adicione a coluna 'Month' com o valor do mês da coluna \"Published At\"\n",
        "df_video = df_video.withColumn(\"Month\", month(col(\"Published At\")))\n",
        "print(\"Coluna 'Month' adicionada:\")\n",
        "df_video.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVe6yw1k_Dug"
      },
      "outputs": [],
      "source": [
        "# 4. Adicione a coluna 'Keyword Index' transformando 'keyword' em valores numéricos\n",
        "\n",
        "# Adicionar uma coluna de exemplo \"keyword\" com valor padrão\n",
        "df_video = df_video.withColumn(\"keyword\", lit(\"default_keyword\"))\n",
        "\n",
        "string_indexer = StringIndexer(inputCol=\"Keyword\", outputCol=\"Keyword Index\")\n",
        "\n",
        "# Renomear a coluna (se necessário)\n",
        "df_video = df_video.withColumnRenamed(\"Keyword com espaço\", \"keyword\")\n",
        "\n",
        "string_indexer = StringIndexer(inputCol=\"keyword\", outputCol=\"Keyword Index\")\n",
        "df_video = string_indexer.fit(df_video).transform(df_video)\n",
        "print(\"Coluna 'Keyword Index' adicionada:\")\n",
        "df_video.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyFrD9fq_M4o"
      },
      "outputs": [],
      "source": [
        "# 5. Crie o vetor \"Features\" com os campos: Likes, Views, Year, Month, Keyword Index\n",
        "# Converter a coluna 'Year' para IntegerType\n",
        "df_video = df_video.withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n",
        "\n",
        "# Verificar se a conversão foi bem-sucedida\n",
        "print(\"Coluna 'Year' convertida para IntegerType:\")\n",
        "df_video.printSchema()\n",
        "\n",
        "# Criar o vetor \"Features\" novamente\n",
        "feature_assembler = VectorAssembler(\n",
        "    inputCols=[\"Likes\", \"Views\", \"Year\", \"Month\", \"Keyword Index\"],\n",
        "    outputCol=\"Features\"\n",
        ")\n",
        "\n",
        "df_video = feature_assembler.transform(df_video)\n",
        "print(\"Coluna 'Features' criada:\")\n",
        "df_video.show(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTCPsNWP_Toi"
      },
      "outputs": [],
      "source": [
        "# 6. Adicione a coluna 'Features Normal' com os dados normalizados\n",
        "normalizer = Normalizer(inputCol=\"Features\", outputCol=\"Features Normal\", p=2.0)\n",
        "df_video = normalizer.transform(df_video.na.drop(subset=[\"Features\"]))\n",
        "print(\"Coluna 'Features Normal' adicionada:\")\n",
        "df_video.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VJa6FFV_bZd"
      },
      "outputs": [],
      "source": [
        "# 7. Adicione a coluna 'Features PCA' com redução de características usando PCA\n",
        "pca = PCA(k=1, inputCol=\"Features\", outputCol=\"Features PCA\")\n",
        "pca_model = pca.fit(df_video)\n",
        "df_video = pca_model.transform(df_video)\n",
        "print(\"Coluna 'Features PCA' adicionada:\")\n",
        "df_video.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Td5djB_jIS"
      },
      "outputs": [],
      "source": [
        "# 8. Separe os dados em treinamento (80%) e teste (20%)\n",
        "train_data, test_data = df_video.randomSplit([0.8, 0.2], seed=42)\n",
        "print(\"Dados de treino:\")\n",
        "train_data.show(10)\n",
        "print(\"Dados de teste:\")\n",
        "test_data.show(10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqXCaVAl_w1F"
      },
      "outputs": [],
      "source": [
        "# 9. Crie e treine o modelo de regressão linear\n",
        "lr = LinearRegression(featuresCol=\"Features Normal\", labelCol=\"Comments\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Avaliação do modelo\n",
        "test_results = lr_model.evaluate(test_data)\n",
        "print(\"R²: {:.4f}\".format(test_results.r2))\n",
        "print(\"RMSE: {:.4f}\".format(test_results.rootMeanSquaredError))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWLTg7RvDD8H"
      },
      "outputs": [],
      "source": [
        "# Salvar o DataFrame em formato Parquet\n",
        "df_video.write.mode(\"overwrite\").parquet(\"videos-preparados-parquet\")\n",
        "\n",
        "# Arquivo Salvo\n",
        "print(\"DataFrame salvo com sucesso em formato Parquet!\")\n",
        "\n",
        "# Finalizar a Spark Session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
